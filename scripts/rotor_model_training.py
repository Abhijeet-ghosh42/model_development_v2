import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


import argparse
import logging
import time
import warnings

import joblib
import mlflow
import numpy as np
import pandas as pd
import yaml
from mlflow_base import MLflowBase
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.svm import OneClassSVM

from data_loaders import load_esa_harmonics_api, load_local_csv

# --- Pre-Script Configuration & Constants ---

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# DATA_DIR = "data"
# ARTIFACTS_DIR = "artifacts"

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(ROOT_DIR, "data")
ARTIFACTS_DIR = os.path.join(ROOT_DIR, "artifacts")


# --- Model Training Class ---

class AdvancedRotorFaultTrainer:
    """A class to encapsulate the rotor fault model training pipeline."""
    def __init__(self, n_components, svm_nu, svm_gamma):
        self.pca = PCA(n_components=n_components)
        self.scaler = StandardScaler()
        self.svm = OneClassSVM(nu=svm_nu, gamma=svm_gamma)
        self.imputer = SimpleImputer(strategy='mean')
    
    def extract_sideband_features(self, harmonics_data):
        """Extracts sideband harmonic features from the dataframe."""
        harmonics_indices = range(1, 31)
        feature_cols = []

        for phase_type in ['ch', 'vh']:
            for h_index in harmonics_indices:
                for phase_num in range(1, 4):
                    col_name = f"{phase_type}{phase_num}_{h_index}"
                    if col_name in harmonics_data.columns:
                        feature_cols.append(harmonics_data[col_name].values)
                    else:
                        logger.warning(f"Column {col_name} missing, filling with NaN.")
                        feature_cols.append(np.full(len(harmonics_data), np.nan))
        
        return np.column_stack(feature_cols)

    def fit(self, harmonics_data):
        """Fits the entire preprocessing and model pipeline."""
        t0 = time.time()
        logger.info("Extracting sideband features...")
        sideband_data = self.extract_sideband_features(harmonics_data)
        
        logger.info("Imputing missing values...")
        imputed_data = self.imputer.fit_transform(sideband_data)
        
        logger.info("Scaling data...")
        scaled_data = self.scaler.fit_transform(imputed_data)
        
        logger.info("Applying PCA...")
        pca_data = self.pca.fit_transform(scaled_data)
        
        logger.info("Fitting One-Class SVM...")
        self.svm.fit(pca_data)
        
        logger.info(f"‚úÖ Total training time: {time.time() - t0:.2f}s")
        return pca_data
        
# --- Helper Functions ---

def generate_run_notes(params, metrics, model_path, yaml_path):
    """Generates a markdown note for the MLflow run."""
    def _fmt(x, nd=4):
        if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))): return "NaN"
        try: return f"{float(x):.{nd}f}"
        except: return str(x)
        
    return f"""
# üåÄ Rotor Fault Detection ‚Äî Run Summary

### Parameters
- **Algorithm**: {params['run_name']}
- **Tenant / Machine**: {params['tenant_id']} / {params['machine_id']}
- **Dataset**: {params['dataset_filename']}
- **PCA components**: {params['pca_n_components']}
- **SVM parameters**: ŒΩ = {params['svm_nu']}, Œ≥ = {params['svm_gamma']}

### Key Metrics
- **Explained variance (PCA)**: {_fmt(metrics['explained_variance'])}
- **SVM score mean**: {_fmt(metrics['score_mean'])}
- **SVM score std**: {_fmt(metrics['score_std'])}

### Artifacts
- **Model file**: `{model_path}`
- **Config YAML**: `{yaml_path}`
"""

# --- Main Execution Logic ---

def main(args):
    """
    Main function to run the rotor fault model training and MLflow logging pipeline.
    """
    if mlflow.active_run():
        mlflow.end_run()

    # 1. MLflow Setup
    experiment_name = f"rotor_fault_monitoring/{args.tenant_id}/{args.machine_id}"
    mlbase = MLflowBase(experiment_name)
    run = mlbase.start_run(run_name=args.run_name)
    run_id = run.info.run_id
    mlflow.set_tag("run_tag", args.run_tag)
    logger.info(f"üöÄ Started MLflow Run: {run_id} in Experiment: '{experiment_name}'")

    try:
        # 2. Log Initial Parameters
        params_to_log = {
            "run_name": args.run_name,
            "dataset_filename": args.dataset_filename,
            "tenant_id": args.tenant_id,
            "machine_id": args.machine_id,
            "pca_n_components": args.n_components,
            "svm_nu": args.svm_nu,
            "svm_gamma": args.svm_gamma,
        }
        mlbase.log_params(params_to_log)

        # 3. Load & Prepare Data
        # dataset_path = os.path.join(DATA_DIR, args.dataset_filename)
        # logger.info(f"üíæ Loading data from: {dataset_path}")
        # if not os.path.exists(dataset_path):
        #     raise FileNotFoundError(f"Dataset not found at {dataset_path}.")
        
        # df = pd.read_csv(dataset_path)
        # drop_cols = [c for c in ["_id", "tenant_id", "machine_id", "type", "timestamp"] if c in df.columns]
        # harmonics_data = df.drop(columns=drop_cols, errors="ignore")
        
        if args.source == "local":
            dataset_path = os.path.join(DATA_DIR, args.dataset_filename)
            logger.info(f"üíæ Loading data (local): {dataset_path}")
            if not os.path.exists(dataset_path):
                raise FileNotFoundError(f"Dataset not found at {dataset_path}.")
            df = load_local_csv(dataset_path)
        else:
            if not all([args.api_base_url, args.api_key, args.start_datetime_utc, args.end_datetime_utc]):
                raise ValueError("For --source api, you must provide --api-base-url, --api-key, --start-datetime-utc, and --end-datetime-utc")
            logger.info(
                f"üåê Loading data (API): {args.api_base_url} | machine_id={args.machine_id} | "
                f"{args.start_datetime_utc} ‚Üí {args.end_datetime_utc}"
            )
            df = load_esa_harmonics_api(
                base_url=args.api_base_url,
                api_key=args.api_key,
                machine_id=args.machine_id,
                start_utc=args.start_datetime_utc,
                end_utc=args.end_datetime_utc,
                endpoint="/api/v1/energy/",
                max_span_hours=args.max_span_hours,
                extra_params={"response_type": "raw"},
            )
            if df.empty:
                raise ValueError("API returned no data for the requested time range.")

        drop_cols = [c for c in ["_id", "tenant_id", "machine_id", "type", "timestamp"] if c in df.columns]
        harmonics_data = df.drop(columns=drop_cols, errors="ignore")


        # 4. Train Model
        trainer = AdvancedRotorFaultTrainer(args.n_components, args.svm_nu, args.svm_gamma)
        
        data_to_train = harmonics_data
        if args.sample_size:
            logger.warning(f"Using a random sample of {args.sample_size} rows for training.")
            data_to_train = harmonics_data.sample(n=args.sample_size, random_state=42)

        pca_data = trainer.fit(data_to_train)
        
        # 5. Calculate Metrics
        scores = trainer.svm.decision_function(pca_data)
        metrics_to_log = {
            "explained_variance": float(trainer.pca.explained_variance_ratio_.sum()),
            "score_mean": float(np.mean(scores)),
            "score_std": float(np.std(scores)),
        }
        mlbase.log_metrics(metrics_to_log)
        logger.info(f"üìä Metrics calculated: Explained Variance = {metrics_to_log['explained_variance']:.4f}")

        # 6. Save Artifacts
        os.makedirs(ARTIFACTS_DIR, exist_ok=True)
        model_filename = f"rotor_fault_detector_{args.machine_id}.pkl"
        model_path = os.path.join(ARTIFACTS_DIR, model_filename)
        joblib.dump((trainer.imputer, trainer.scaler, trainer.pca, trainer.svm), model_path)
        
        config = {
            "tenant_id": args.tenant_id,
            "machine_id": args.machine_id,
            "pca_n_components": args.n_components,
            "svm_nu": args.svm_nu,
            "svm_gamma": args.svm_gamma,
            "decision_threshold": float(np.percentile(scores, 5)),
            "model_path": model_filename,
        }
        yaml_filename = f"rotor_fault_config_{args.machine_id}.yaml"
        yaml_path = os.path.join(ARTIFACTS_DIR, yaml_filename)
        with open(yaml_path, "w") as f:
            yaml.dump(config, f, sort_keys=False)

        # 7. Log Artifacts to MLflow
        mlbase.log_artifact(run_id, local_path=model_path)
        mlbase.log_artifact(run_id, local_path=yaml_path)
        logger.info(f"üìú Saved and logged artifacts to MLflow: {model_filename}, {yaml_filename}")
        
        # 8. Set Run Notes
        notes = generate_run_notes(params_to_log, metrics_to_log, model_filename, yaml_filename)
        mlflow.set_tag("mlflow.note.content", notes)

    except Exception as e:
        logger.error(f"‚ùå An error occurred: {e}", exc_info=True)
        raise
    finally:
        mlflow.end_run()
        logger.info("üèÅ Run finished.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train a Rotor Fault Detection model and log to MLflow.")

    # example usage production:
    # python rotor_model_training.py --tenant-id "28" --machine-id "257" --dataset-filename "iotts.harmonics_257.csv" --run-tag @production
    # example usage testing:
    # python rotor_model_training.py --tenant-id "28" --machine-id "257" --dataset-filename "iotts.harmonics_257.csv" --run-tag @testing

    # example usage with API source of data:
    # python rotor_model_training.py --tenant-id 28 --machine-id 257 --dataset-filename ignore.csv --source api --api-base-url "https://iot.zolnoi.app" --api-key "mqdm_CgwNaRsb62Ziy5ePw" --start-datetime-utc "2025-08-01T01:30:00Z" --end-datetime-utc "2025-08-02T01:30:00Z"


    # Required Arguments
    parser.add_argument("--tenant-id", type=str, required=True, help="Tenant ID.")
    parser.add_argument("--machine-id", type=str, required=True, help="Machine ID.")
    parser.add_argument("--dataset-filename", type=str, required=True, help=f"Dataset CSV filename in '{DATA_DIR}/'.")
    
    # Optional Model/Knob Arguments
    parser.add_argument("--run-name", type=str, default="pca_ocsvm_sidebands_v1", help="Name for the MLflow run.")
    parser.add_argument("--n-components", type=int, default=5, help="Number of PCA components.")
    parser.add_argument("--svm-nu", type=float, default=0.05, help="Nu parameter for OneClassSVM.")
    parser.add_argument("--svm-gamma", type=str, default="scale", help="Gamma parameter for OneClassSVM.")
    parser.add_argument("--sample-size", type=int, default=None, help="Optional: use a random sample of N rows for quick training.")

    parser.add_argument("--run-tag", type=str, choices=["@production", "@testing"], default="@testing", help="Tag for the MLflow run: '@production' or '@testing'.")

    # --- Data source toggle ---
    parser.add_argument("--source", choices=["local", "api"], default="local",
                        help="Where to load data from (default: local CSV).")
    parser.add_argument("--api-base-url", type=str, default=os.getenv("IOT_API_BASE_URL"),
                        help="API base URL (e.g., https://iot.zolnoi.app). Defaults to env IOT_API_BASE_URL.")
    parser.add_argument("--api-key", type=str, default=os.getenv("IOT_API_KEY"),
                        help="API key. Defaults to env IOT_API_KEY.")
    parser.add_argument("--start-datetime-utc", type=str, default=None,
                        help="UTC ISO8601 start (e.g., 2025-08-01T01:30:00Z). Required if --source api.")
    parser.add_argument("--end-datetime-utc", type=str, default=None,
                        help="UTC ISO8601 end (e.g., 2025-08-02T01:30:00Z). Required if --source api.")
    parser.add_argument("--max-span-hours", type=int, default=24,
                        help="Max hours per API chunk (‚â§24).")

    cli_args = parser.parse_args()
    main(cli_args)
