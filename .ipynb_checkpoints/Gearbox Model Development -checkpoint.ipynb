{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0946a34e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing required environment variables. Please check .env.local file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mend_run()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Start run\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m mlbase \u001b[38;5;241m=\u001b[39m \u001b[43mMLflowBase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEXPERIMENT_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m run \u001b[38;5;241m=\u001b[39m mlbase\u001b[38;5;241m.\u001b[39mstart_run(run_name\u001b[38;5;241m=\u001b[39mRUN_NAME)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLflow run:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mlflow\u001b[38;5;241m.\u001b[39mactive_run()\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id)\n",
      "File \u001b[1;32mC:\\Projects\\model_development\\model_development_v2\\model_development_v2\\mlflow_base.py:38\u001b[0m, in \u001b[0;36mMLflowBase.__init__\u001b[1;34m(self, experiment_name)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassword \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLFLOW_TRACKING_PASSWORD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracking_uri, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musername, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassword]):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required environment variables. Please check .env.local file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Set tracking URI\u001b[39;00m\n\u001b[0;32m     43\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracking_uri)\n",
      "\u001b[1;31mValueError\u001b[0m: Missing required environment variables. Please check .env.local file."
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Setup & MLflow ---\n",
    "import os, re, json, math, yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow_base import MLflowBase\n",
    "\n",
    "# Experiment / run naming\n",
    "EXPERIMENT_NAME = \"gearbox_fault_monitoring/28/257\"\n",
    "RUN_NAME        = \"harmonic_profiling_kstest_v1\"   # algorithm/approach name\n",
    "\n",
    "# End any existing active run (notebook safety)\n",
    "if mlflow.active_run() is not None:\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Start run\n",
    "mlbase = MLflowBase(EXPERIMENT_NAME)\n",
    "run = mlbase.start_run(run_name=RUN_NAME)\n",
    "print(\"MLflow run:\", mlflow.active_run().info.run_id)\n",
    "\n",
    "# ---------- Core params we want to track ----------\n",
    "# Data & IDs\n",
    "dataset_path = \"iotts.harmonics_253.csv\" # or dataset id\n",
    "tenant_id    = \"28\"\n",
    "machine_id   = \"257\"\n",
    "\n",
    "# Algorithm/config knobs\n",
    "stoppage_current_threshold = 40\n",
    "low_order_range, intermediate_order_range, high_order_range = (1,10), (11,20), (21,30)\n",
    "use_parity_profiles = True\n",
    "\n",
    "# Unlabeled evaluation settings\n",
    "BASELINE_FRACTION = 0.7                  # healthy portion\n",
    "AGGREGATION       = \"max_z\"              # \"max_z\" or \"mean_z\"\n",
    "THRESHOLD_METHOD  = \"mean+3sigma\"        # or \"p99\"\n",
    "\n",
    "# Logging important params\n",
    "mlbase.log_params({\n",
    "    \"algorithm_name\": RUN_NAME,\n",
    "    \"dataset_path\": os.path.basename(dataset_path),\n",
    "    \"tenant_id\": tenant_id,\n",
    "    \"machine_id\": machine_id,\n",
    "    \"stoppage_current_threshold\": stoppage_current_threshold,\n",
    "    \"low_order_range\": str(low_order_range),\n",
    "    \"intermediate_order_range\": str(intermediate_order_range),\n",
    "    \"high_order_range\": str(high_order_range),\n",
    "    \"use_parity_profiles\": use_parity_profiles,\n",
    "    \"eval_baseline_fraction\": BASELINE_FRACTION,\n",
    "    \"eval_aggregation\": AGGREGATION,\n",
    "    \"eval_threshold_method\": THRESHOLD_METHOD,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e11b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "\n",
    "# # Addding a note to document what is being stored and it's significance\n",
    "# mlflow.set_tag(\n",
    "#     \"mlflow.note.content\",\n",
    "#     \"\"\"\n",
    "# ### Run Documentation\n",
    "# **what's being stored and their significance.\n",
    "\n",
    "# #### Parameters (Why they matter)\n",
    "# - **stoppage_current_threshold**: Ensures machine is running; avoids false analysis during stoppage.  \n",
    "# - **low/intermediate/high_order_range**: Defines harmonic groupings for profiling faults.  \n",
    "# - **use_parity_profiles**: Includes odd/even THD groupings, important for detecting asymmetry in harmonics.  \n",
    "\n",
    "# #### Metrics (Significance)\n",
    "# - **current_imbalance_mean/std**: Captures average imbalance across phases; high values ‚Üí winding/connection issues.  \n",
    "# - **voltage_imbalance_mean/std**: Highlights phase imbalance on supply side.  \n",
    "# - **harmonic_threshold**: Aggregate harmonic activity, key for detecting gearbox stress.  \n",
    "# - **statistical_threshold**: Statistical baseline check for abnormal deviation.  \n",
    "# - **drift_cohens_d**: Measures how much recent data distribution shifts from baseline.  \n",
    "# - **far_healthy**: False-alarm rate when system is presumed healthy.  \n",
    "# - **arl0_samples**: Average samples between false alarms (reliability measure).  \n",
    "\n",
    "# #### Decision Context\n",
    "# - Config is considered reliable if  \n",
    "#   - `far_healthy < 1%` (low false alarms), and  \n",
    "#   - `drift_cohens_d > 0.8` (strong signal vs baseline).  \n",
    "\n",
    "# ---\n",
    "# *Note: This run uses `max_z` aggregation and `mean+3œÉ` thresholding.*  \n",
    "# \"\"\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e07bb22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Cell X: Attach rich run documentation (Notes) ---\n",
    "# import mlflow\n",
    "# import numpy as np\n",
    "\n",
    "# def _fmt(x, nd=4):\n",
    "#     if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):\n",
    "#         return \"NaN\"\n",
    "#     try:\n",
    "#         return f\"{float(x):.{nd}f}\"\n",
    "#     except Exception:\n",
    "#         return str(x)\n",
    "\n",
    "# mlflow.set_tag(\n",
    "#     \"mlflow.note.content\",\n",
    "# f\"\"\"\n",
    "# ### Run Documentation\n",
    "\n",
    "# This note summarizes **what we logged** in MLflow for quick comparison across runs and why each item matters.\n",
    "\n",
    "# ---\n",
    "\n",
    "# #### üì¶ Artifact (the deliverable)\n",
    "# - **Config YAML**: `{output_yaml}`  \n",
    "#   Contains per-profile normalization pairs `[mean, std]` and `stoppage_current_threshold`.  \n",
    "#   This is the file used by the online detector.\n",
    "\n",
    "# ---\n",
    "\n",
    "# #### ‚öôÔ∏è Parameters (setup & reproducibility)\n",
    "# - **algorithm_name**: `{RUN_NAME}` ‚Äî approach used to generate the config.\n",
    "# - **tenant_id / machine_id**: `{tenant_id}` / `{machine_id}` ‚Äî routing & traceability.\n",
    "# - **dataset_path**: `{os.path.basename(dataset_path)}` ‚Äî source data identifier.\n",
    "# - **stoppage_current_threshold**: `{stoppage_current_threshold}` ‚Äî filters non-operating periods.\n",
    "# - **harmonic buckets**: low `{low_order_range}`, intermediate `{intermediate_order_range}`, high `{high_order_range}`.\n",
    "# - **use_parity_profiles**: `{use_parity_profiles}` ‚Äî include odd/even profiles.\n",
    "# - **eval_baseline_fraction**: `{BASELINE_FRACTION}` ‚Äî portion assumed healthy to derive baseline.\n",
    "# - **eval_aggregation**: `{AGGREGATION}` ‚Äî how profiles combine into one fault score.\n",
    "# - **eval_threshold_method**: `{THRESHOLD_METHOD}` ‚Äî rule for decision threshold on baseline.\n",
    "# - **profiles_used**: `{profiles_used}` ‚Äî number of profiles with usable data.\n",
    "# - **eval_decision_threshold**: `{_fmt(thr)}` ‚Äî derived operating threshold for the fault score.\n",
    "\n",
    "# ---\n",
    "\n",
    "# #### üìä Core Metrics (for benchmarking without labels)\n",
    "# - **missing_fraction_overall**: `{_fmt(missing_fraction_overall)}` ‚Äî overall data completeness.\n",
    "# - **coverage_valid_row_fraction_min**: `{_fmt(coverage_min)}` ‚Äî worst coverage across profiles.\n",
    "\n",
    "# **Separability & shift**\n",
    "# - **drift_cohens_d**: `{_fmt(cohen_d)}` ‚Äî effect size between recent vs baseline fault scores.\n",
    "# - **psi_fault_score**: `{_fmt(psi_val)}` ‚Äî population stability index for fault score.\n",
    "\n",
    "# **False-alarm behavior**\n",
    "# - **far_healthy**: `{_fmt(far_healthy)}` ‚Äî fraction of baseline flagged.\n",
    "# - **arl0_samples**: `{_fmt(arl0)}` ‚Äî expected samples between false alarms.\n",
    "\n",
    "# **Operational recent behavior**\n",
    "# - **recent_pct_time_anomalous**: `{_fmt(pct_time_anom_recent)}` ‚Äî fraction of recent period flagged.\n",
    "\n",
    "# **Baseline stability**\n",
    "# - **mean_profile_cv_baseline**: `{_fmt(mean_profile_cv_baseline)}` ‚Äî average coefficient of variation across profiles in healthy window.\n",
    "\n",
    "# **Single ranking score**\n",
    "# - **quality_index**: `{_fmt(quality_index)}`  \n",
    "#   Formula: `cohen_d - 5*far_healthy + 0.5*psi - 0.5*mean_profile_cv_baseline`.\n",
    "\n",
    "# ---\n",
    "\n",
    "# #### ‚úÖ Promotion guideline\n",
    "# Promote when:\n",
    "# - `far_healthy < 0.01` and `arl0_samples ‚â• 100`, **and**\n",
    "# - `drift_cohens_d ‚â• 0.8`, **and**\n",
    "# - `coverage_valid_row_fraction_min ‚â• 0.8`, **and**\n",
    "# - `quality_index` ranks near the top among runs for this machine/config family.\n",
    "\n",
    "# *Aggregation:* `{AGGREGATION}` ¬∑ *Thresholding:* `{THRESHOLD_METHOD}` ¬∑ *Baseline fraction:* `{BASELINE_FRACTION}`\n",
    "# \"\"\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0d93da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shape: (14491, 91)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>vh1_0</th>\n",
       "      <th>vh2_9</th>\n",
       "      <th>vh2_8</th>\n",
       "      <th>vh2_0</th>\n",
       "      <th>vh1_2</th>\n",
       "      <th>vh3_7</th>\n",
       "      <th>ch1_13</th>\n",
       "      <th>vh3_11</th>\n",
       "      <th>ch1_7</th>\n",
       "      <th>...</th>\n",
       "      <th>ch3_12</th>\n",
       "      <th>ch2_5</th>\n",
       "      <th>vh1_7</th>\n",
       "      <th>vh2_13</th>\n",
       "      <th>ch2_10</th>\n",
       "      <th>ch2_11</th>\n",
       "      <th>vh1_6</th>\n",
       "      <th>ch1_0</th>\n",
       "      <th>ch1_4</th>\n",
       "      <th>ch2_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-02 08:00:08+00:00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686252</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.646852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.689831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-02 08:02:17+00:00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605559</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.777842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.800501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-02 08:03:26+00:00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.579840</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.755995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.815620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  vh1_0  vh2_9     vh2_8  vh2_0     vh1_2  vh3_7  \\\n",
       "0 2025-08-02 08:00:08+00:00  100.0    0.0  0.686252  100.0  0.646852    0.0   \n",
       "1 2025-08-02 08:02:17+00:00  100.0    0.0  0.605559  100.0  0.777842    0.0   \n",
       "2 2025-08-02 08:03:26+00:00  100.0    0.0  0.579840  100.0  0.755995    0.0   \n",
       "\n",
       "   ch1_13  vh3_11  ch1_7  ...  ch3_12  ch2_5  vh1_7  vh2_13  ch2_10  ch2_11  \\\n",
       "0     0.0     0.0    0.0  ...     0.0    0.0    0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0    0.0  ...     0.0    0.0    0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0    0.0  ...     0.0    0.0    0.0     0.0     0.0     0.0   \n",
       "\n",
       "      vh1_6  ch1_0  ch1_4  ch2_8  \n",
       "0  1.689831    0.0    0.0    0.0  \n",
       "1  1.800501    0.0    0.0    0.0  \n",
       "2  1.815620    0.0    0.0    0.0  \n",
       "\n",
       "[3 rows x 91 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Cell 2: Load & basic health ---\n",
    "df = pd.read_csv(dataset_path)\n",
    "if \"timestamp\" in df.columns:\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "drop_cols = [c for c in [\"_id\",\"metaData.tenant_id\",\"metaData.machine_id\"] if c in df.columns]\n",
    "if drop_cols:\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "display(df.head(3))\n",
    "\n",
    "missing_fraction_overall = float(df.isna().mean().mean()) if df.size else math.nan\n",
    "mlbase.log_metrics({\n",
    "    \"n_rows\": float(df.shape[0]),\n",
    "    \"n_cols\": float(df.shape[1]),\n",
    "    \"missing_fraction_overall\": missing_fraction_overall\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eec82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Column parsing helpers ---\n",
    "def extract_info(col_name: str):\n",
    "    \"\"\"\n",
    "    Parse names like 'ch1_7' or 'vh3_12' => (type in {'c','v'}, phase in {1,2,3}, harmonic order int)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = col_name.split(\"_\")\n",
    "        type_phase_str = parts[0]\n",
    "        freq = int(parts[1])\n",
    "        m = re.match(r\"([a-z]+)([0-9]+)\", type_phase_str, re.I)\n",
    "        if not m: return (None, None, None)\n",
    "        type_str, phase_str = m.groups()\n",
    "        t = \"c\" if type_str.lower()==\"ch\" else \"v\" if type_str.lower()==\"vh\" else None\n",
    "        ph = int(phase_str)\n",
    "        return (t, ph, freq) if t in {\"c\",\"v\"} else (None, None, None)\n",
    "    except Exception:\n",
    "        return (None, None, None)\n",
    "\n",
    "def filter_harmonic_cols(df, t=\"c\", phase=None, order_range=None, parity=None):\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        tt, ph, fr = extract_info(col)\n",
    "        if tt != t or fr is None: continue\n",
    "        if phase is not None and ph != phase: continue\n",
    "        if order_range is not None:\n",
    "            lo, hi = order_range\n",
    "            if not (lo <= fr <= hi): continue\n",
    "        if parity == \"odd\"  and fr % 2 == 0: continue\n",
    "        if parity == \"even\" and fr % 2 != 0: continue\n",
    "        cols.append(col)\n",
    "    return cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bc32993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Cell 4: Profiles & tiny diagnostics ---\n",
    "# def rms_across_harmonics(subdf: pd.DataFrame) -> pd.Series:\n",
    "#     if subdf.empty: return pd.Series([], dtype=float)\n",
    "#     vals = subdf.to_numpy(dtype=float)\n",
    "#     with np.errstate(invalid=\"ignore\"):\n",
    "#         return pd.Series(np.sqrt(np.nanmean(np.square(vals), axis=1)), index=subdf.index)\n",
    "\n",
    "# def build_profile(df, t=\"c\", order_range=None, parity=None):\n",
    "#     phase_series, debug = [], {}\n",
    "#     for ph in [1,2,3]:\n",
    "#         cols = filter_harmonic_cols(df, t=t, phase=ph, order_range=order_range, parity=parity)\n",
    "#         debug[f\"phase{ph}_ncols\"] = len(cols)\n",
    "#         phase_series.append(rms_across_harmonics(df[cols]) if cols else pd.Series(index=df.index, dtype=float))\n",
    "#     stacked = pd.concat(phase_series, axis=1) if phase_series else pd.DataFrame(index=df.index)\n",
    "#     profile = stacked.mean(axis=1, skipna=True) if not stacked.empty else pd.Series(index=df.index, dtype=float)\n",
    "#     debug[\"rows\"] = int(df.shape[0])\n",
    "#     debug[\"valid_rows\"] = int(profile.dropna().shape[0])\n",
    "#     return profile, debug\n",
    "\n",
    "# profiles_to_compute = {\n",
    "#     \"low_order_current_harmonics\":         dict(t=\"c\", order_range=(1,10),  parity=None),\n",
    "#     \"intermediate_order_current_harmonics\":dict(t=\"c\", order_range=(11,20), parity=None),\n",
    "#     \"high_order_current_harmonics\":        dict(t=\"c\", order_range=(21,30), parity=None),\n",
    "# }\n",
    "# if use_parity_profiles:\n",
    "#     profiles_to_compute.update({\n",
    "#         \"odd_parity_current_harmonics\":  dict(t=\"c\", order_range=None, parity=\"odd\"),\n",
    "#         \"even_parity_current_harmonics\": dict(t=\"c\", order_range=None, parity=\"even\"),\n",
    "#     })\n",
    "\n",
    "# profile_series_map, profile_debug_map = {}, {}\n",
    "# for pname, args in profiles_to_compute.items():\n",
    "#     s, dbg = build_profile(df, **args)\n",
    "#     profile_series_map[pname] = s\n",
    "#     profile_debug_map[pname] = dbg\n",
    "#     # Minimal, comparable metrics per profile: coverage & richness\n",
    "#     total_ncols = sum(dbg.get(f\"phase{ph}_ncols\",0) for ph in [1,2,3])\n",
    "#     valid_frac  = (dbg[\"valid_rows\"]/dbg[\"rows\"]) if dbg[\"rows\"]>0 else float(\"nan\")\n",
    "#     mlbase.log_metrics({\n",
    "#         f\"{pname}_valid_row_fraction\": valid_frac,\n",
    "#         f\"{pname}_ncols_total\": float(total_ncols),\n",
    "#     })\n",
    "#     if dbg[\"valid_rows\"] < 50:\n",
    "#         print(f\"WARNING: {pname} has only {dbg['valid_rows']} valid rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edf7980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghosh\\AppData\\Local\\Temp\\ipykernel_28848\\202380440.py:6: RuntimeWarning: Mean of empty slice\n",
      "  return pd.Series(np.sqrt(np.nanmean(np.square(vals), axis=1)), index=subdf.index)\n",
      "C:\\Users\\ghosh\\AppData\\Local\\Temp\\ipykernel_28848\\202380440.py:6: RuntimeWarning: Mean of empty slice\n",
      "  return pd.Series(np.sqrt(np.nanmean(np.square(vals), axis=1)), index=subdf.index)\n",
      "C:\\Users\\ghosh\\AppData\\Local\\Temp\\ipykernel_28848\\202380440.py:6: RuntimeWarning: Mean of empty slice\n",
      "  return pd.Series(np.sqrt(np.nanmean(np.square(vals), axis=1)), index=subdf.index)\n",
      "C:\\Users\\ghosh\\AppData\\Local\\Temp\\ipykernel_28848\\202380440.py:6: RuntimeWarning: Mean of empty slice\n",
      "  return pd.Series(np.sqrt(np.nanmean(np.square(vals), axis=1)), index=subdf.index)\n",
      "C:\\Users\\ghosh\\AppData\\Local\\Temp\\ipykernel_28848\\202380440.py:6: RuntimeWarning: Mean of empty slice\n",
      "  return pd.Series(np.sqrt(np.nanmean(np.square(vals), axis=1)), index=subdf.index)\n",
      "C:\\Users\\ghosh\\AppData\\Local\\Temp\\ipykernel_28848\\202380440.py:6: RuntimeWarning: Mean of empty slice\n",
      "  return pd.Series(np.sqrt(np.nanmean(np.square(vals), axis=1)), index=subdf.index)\n",
      "C:\\Users\\ghosh\\AppData\\Local\\Temp\\ipykernel_28848\\202380440.py:6: RuntimeWarning: Mean of empty slice\n",
      "  return pd.Series(np.sqrt(np.nanmean(np.square(vals), axis=1)), index=subdf.index)\n",
      "C:\\Users\\ghosh\\AppData\\Local\\Temp\\ipykernel_28848\\202380440.py:6: RuntimeWarning: Mean of empty slice\n",
      "  return pd.Series(np.sqrt(np.nanmean(np.square(vals), axis=1)), index=subdf.index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: high_order_current_harmonics has only 0 valid rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4 (revised): Profiles & compact coverage logging ---\n",
    "def rms_across_harmonics(subdf: pd.DataFrame) -> pd.Series:\n",
    "    if subdf.empty: return pd.Series([], dtype=float)\n",
    "    vals = subdf.to_numpy(dtype=float)\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        return pd.Series(np.sqrt(np.nanmean(np.square(vals), axis=1)), index=subdf.index)\n",
    "\n",
    "def build_profile(df, t=\"c\", order_range=None, parity=None):\n",
    "    phase_series, debug = [], {}\n",
    "    for ph in [1,2,3]:\n",
    "        cols = filter_harmonic_cols(df, t=t, phase=ph, order_range=order_range, parity=parity)\n",
    "        debug[f\"phase{ph}_ncols\"] = len(cols)\n",
    "        phase_series.append(rms_across_harmonics(df[cols]) if cols else pd.Series(index=df.index, dtype=float))\n",
    "    stacked = pd.concat(phase_series, axis=1) if phase_series else pd.DataFrame(index=df.index)\n",
    "    profile = stacked.mean(axis=1, skipna=True) if not stacked.empty else pd.Series(index=df.index, dtype=float)\n",
    "    debug[\"rows\"] = int(df.shape[0])\n",
    "    debug[\"valid_rows\"] = int(profile.dropna().shape[0])\n",
    "    return profile, debug\n",
    "\n",
    "profiles_to_compute = {\n",
    "    \"low_order_current_harmonics\":          dict(t=\"c\", order_range=(1,10),  parity=None),\n",
    "    \"intermediate_order_current_harmonics\": dict(t=\"c\", order_range=(11,20), parity=None),\n",
    "    \"high_order_current_harmonics\":         dict(t=\"c\", order_range=(21,30), parity=None),\n",
    "}\n",
    "if use_parity_profiles:\n",
    "    profiles_to_compute.update({\n",
    "        \"odd_parity_current_harmonics\":  dict(t=\"c\", order_range=None, parity=\"odd\"),\n",
    "        \"even_parity_current_harmonics\": dict(t=\"c\", order_range=None, parity=\"even\"),\n",
    "    })\n",
    "\n",
    "profile_series_map, profile_debug_map = {}, {}\n",
    "profile_valid_fracs = []\n",
    "profiles_used = 0\n",
    "\n",
    "for pname, args in profiles_to_compute.items():\n",
    "    s, dbg = build_profile(df, **args)\n",
    "    profile_series_map[pname] = s\n",
    "    profile_debug_map[pname] = dbg\n",
    "\n",
    "    if dbg[\"rows\"] > 0:\n",
    "        valid_frac = dbg[\"valid_rows\"] / dbg[\"rows\"]\n",
    "        profile_valid_fracs.append(valid_frac)\n",
    "    if dbg[\"valid_rows\"] > 0:\n",
    "        profiles_used += 1\n",
    "\n",
    "# Aggregate coverage signals (compact)\n",
    "coverage_min = float(np.min(profile_valid_fracs)) if profile_valid_fracs else float(\"nan\")\n",
    "mlbase.log_metrics({\"coverage_valid_row_fraction_min\": coverage_min})\n",
    "mlbase.log_params({\"profiles_used\": int(profiles_used)})\n",
    "\n",
    "# Optional: warn if any profile has very low valid rows\n",
    "for pname, dbg in profile_debug_map.items():\n",
    "    if dbg[\"valid_rows\"] < 50:\n",
    "        print(f\"WARNING: {pname} has only {dbg['valid_rows']} valid rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f506df30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated Gearbox Fault Detection Config (YAML):\n",
      "stoppage_current_threshold: 40\n",
      "norm_args:\n",
      "  low_order_current_harmonics:\n",
      "  - 0.521031\n",
      "  - 0.783555\n",
      "  intermediate_order_current_harmonics:\n",
      "  - 0.09268\n",
      "  - 0.153546\n",
      "  high_order_current_harmonics:\n",
      "  - 0.0\n",
      "  - 0.0\n",
      "  odd_parity_current_harmonics:\n",
      "  - 0.027823\n",
      "  - 0.043371\n",
      "  even_parity_current_harmonics:\n",
      "  - 12.519126\n",
      "  - 16.92035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5: Build config & save YAML (artifact) ---\n",
    "def safe_pair(mean_val, std_val):\n",
    "    m = float(mean_val) if not np.isnan(mean_val) else 0.0\n",
    "    s = float(std_val)  if not np.isnan(std_val)  else 0.0\n",
    "    return [round(m,6), round(s,6)]\n",
    "\n",
    "# Compute mean/std per profile\n",
    "profile_stats_map = {\n",
    "    p: {\n",
    "        \"mean\": float(v.dropna().mean()) if v.dropna().size else float(\"nan\"),\n",
    "        \"std\":  float(v.dropna().std(ddof=0)) if v.dropna().size>1 else 0.0\n",
    "    }\n",
    "    for p, v in profile_series_map.items()\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"stoppage_current_threshold\": stoppage_current_threshold,\n",
    "    \"norm_args\": {\n",
    "        \"low_order_current_harmonics\":          safe_pair(profile_stats_map[\"low_order_current_harmonics\"][\"mean\"],          profile_stats_map[\"low_order_current_harmonics\"][\"std\"]),\n",
    "        \"intermediate_order_current_harmonics\": safe_pair(profile_stats_map[\"intermediate_order_current_harmonics\"][\"mean\"], profile_stats_map[\"intermediate_order_current_harmonics\"][\"std\"]),\n",
    "        \"high_order_current_harmonics\":         safe_pair(profile_stats_map[\"high_order_current_harmonics\"][\"mean\"],         profile_stats_map[\"high_order_current_harmonics\"][\"std\"]),\n",
    "    }\n",
    "}\n",
    "if use_parity_profiles:\n",
    "    config[\"norm_args\"][\"odd_parity_current_harmonics\"]  = safe_pair(profile_stats_map[\"odd_parity_current_harmonics\"][\"mean\"],  profile_stats_map[\"odd_parity_current_harmonics\"][\"std\"])\n",
    "    config[\"norm_args\"][\"even_parity_current_harmonics\"] = safe_pair(profile_stats_map[\"even_parity_current_harmonics\"][\"mean\"], profile_stats_map[\"even_parity_current_harmonics\"][\"std\"])\n",
    "\n",
    "output_yaml = f\"gearbox_fault_configs_{machine_id}.yaml\"\n",
    "with open(output_yaml, \"w\") as f:\n",
    "    yaml.dump(config, f, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "print(\"‚úÖ Generated Gearbox Fault Detection Config (YAML):\")\n",
    "print(yaml.dump(config, sort_keys=False, default_flow_style=False))\n",
    "\n",
    "# Log the YAML artifact (the deliverable)\n",
    "mlbase.log_artifact(run.info.run_id, local_path=output_yaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18be01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Cell 6: Unlabeled evaluation (minimal but powerful metrics) ---\n",
    "# from scipy.stats import ks_2samp\n",
    "\n",
    "# # 1) Composite fault score from profiles (z-normalized on baseline)\n",
    "# profiles_df = pd.DataFrame(profile_series_map).sort_index()\n",
    "# n = len(profiles_df)\n",
    "# split_idx = int(n * BASELINE_FRACTION)\n",
    "\n",
    "# baseline_df = profiles_df.iloc[:split_idx].copy()\n",
    "# recent_df   = profiles_df.iloc[split_idx:].copy()\n",
    "\n",
    "# baseline_means = baseline_df.mean(skipna=True)\n",
    "# baseline_stds  = baseline_df.std(skipna=True).replace(0, np.nan)\n",
    "\n",
    "# z_df     = (profiles_df - baseline_means) / baseline_stds\n",
    "# z_pos_df = z_df.clip(lower=0)\n",
    "\n",
    "# if AGGREGATION == \"mean_z\":\n",
    "#     fault_score = z_pos_df.mean(axis=1, skipna=True)\n",
    "# else:  # default \"max_z\"\n",
    "#     fault_score = z_pos_df.max(axis=1,  skipna=True)\n",
    "\n",
    "# baseline_scores = fault_score.iloc[:split_idx].dropna()\n",
    "# recent_scores   = fault_score.iloc[split_idx:].dropna()\n",
    "\n",
    "# # 2) Threshold from baseline\n",
    "# if THRESHOLD_METHOD == \"p99\":\n",
    "#     thr = float(np.nanpercentile(baseline_scores, 99))\n",
    "# else:  # \"mean+3sigma\"\n",
    "#     thr = float(baseline_scores.mean() + 3*baseline_scores.std())\n",
    "\n",
    "# mlbase.log_params({\"eval_decision_threshold\": thr})\n",
    "\n",
    "# # 3) Unlabeled metrics\n",
    "# def cohens_d(a, b):\n",
    "#     a, b = np.asarray(a, float), np.asarray(b, float)\n",
    "#     a, b = a[~np.isnan(a)], b[~np.isnan(b)]\n",
    "#     if a.size < 2 or b.size < 2: return float(\"nan\")\n",
    "#     m1, m2 = a.mean(), b.mean()\n",
    "#     s = np.sqrt(((a.var(ddof=1) + b.var(ddof=1)) / 2.0))\n",
    "#     return float((m2 - m1) / s) if s > 0 else float(\"nan\")\n",
    "\n",
    "# def hellinger_from_hist(a, b, bins=50, rng=None):\n",
    "#     a = np.asarray(a, float); b = np.asarray(b, float)\n",
    "#     a = a[~np.isnan(a)]; b = b[~np.isnan(b)]\n",
    "#     if a.size < 2 or b.size < 2: return float(\"nan\")\n",
    "#     pa, _ = np.histogram(a, bins=bins, range=rng, density=True)\n",
    "#     pb, _ = np.histogram(b, bins=bins, range=rng, density=True)\n",
    "#     pa = pa/(pa.sum()+1e-12); pb = pb/(pb.sum()+1e-12)\n",
    "#     return float(np.sqrt(0.5 * np.sum((np.sqrt(pa) - np.sqrt(pb))**2)))\n",
    "\n",
    "# def psi(a, b, bins=10):\n",
    "#     a = np.asarray(a, float); b = np.asarray(b, float)\n",
    "#     a = a[~np.isnan(a)]; b = b[~np.isnan(b)]\n",
    "#     if a.size < 10 or b.size < 10: return float(\"nan\")\n",
    "#     qs = np.quantile(a, np.linspace(0,1,bins+1)); qs[0], qs[-1] = -np.inf, np.inf\n",
    "#     val = 0.0\n",
    "#     for i in range(bins):\n",
    "#         p = ((a>=qs[i]) & (a<qs[i+1])).mean()\n",
    "#         q = ((b>=qs[i]) & (b<qs[i+1])).mean()\n",
    "#         p = max(p,1e-6); q = max(q,1e-6)\n",
    "#         val += (q-p) * np.log(q/p)\n",
    "#     return float(val)\n",
    "\n",
    "# # Drift / separability\n",
    "# cohen_d = cohens_d(baseline_scores, recent_scores)\n",
    "# hell    = hellinger_from_hist(baseline_scores, recent_scores, bins=50)\n",
    "# ks_stat, ks_p = ks_2samp(baseline_scores, recent_scores)\n",
    "\n",
    "# # False alarms on healthy portion & expected run length\n",
    "# far_healthy = float(np.mean(baseline_scores >= thr))\n",
    "# arl0        = float(1.0 / max(far_healthy, 1e-9))\n",
    "\n",
    "# # Operational recent behavior (no labels)\n",
    "# recent_flags = (recent_scores >= thr).astype(int)\n",
    "# pct_time_anom_recent = float(recent_flags.mean())\n",
    "\n",
    "# # Stability summary of profiles on baseline: mean CV across profiles\n",
    "# cv_vals = []\n",
    "# for p in baseline_df.columns:\n",
    "#     mu = baseline_df[p].mean(skipna=True)\n",
    "#     sd = baseline_df[p].std(skipna=True)\n",
    "#     cv = (sd/mu) if (mu not in [0, np.nan] and not np.isnan(mu)) else np.nan\n",
    "#     if not np.isnan(cv): cv_vals.append(cv)\n",
    "# mean_profile_cv_baseline = float(np.mean(cv_vals)) if cv_vals else float(\"nan\")\n",
    "\n",
    "# # PSI between baseline & recent fault score\n",
    "# psi_val = psi(baseline_scores, recent_scores, bins=10)\n",
    "\n",
    "# # Log ONLY the high-signal metrics\n",
    "# mlbase.log_metrics({\n",
    "#     \"drift_cohens_d\": cohen_d,\n",
    "#     \"drift_hellinger\": hell,\n",
    "#     \"drift_ks_pvalue\": float(ks_p),\n",
    "#     \"far_healthy\": far_healthy,\n",
    "#     \"arl0_samples\": arl0,\n",
    "#     \"recent_pct_time_anomalous\": pct_time_anom_recent,\n",
    "#     \"psi_fault_score\": psi_val,\n",
    "#     \"mean_profile_cv_baseline\": mean_profile_cv_baseline\n",
    "# })\n",
    "\n",
    "# print(\"Logged metrics:\",\n",
    "#       {k: v for k, v in {\n",
    "#           \"drift_cohens_d\": cohen_d,\n",
    "#           \"drift_hellinger\": hell,\n",
    "#           \"drift_ks_pvalue\": float(ks_p),\n",
    "#           \"far_healthy\": far_healthy,\n",
    "#           \"arl0_samples\": arl0,\n",
    "#           \"recent_pct_time_anomalous\": pct_time_anom_recent,\n",
    "#           \"psi_fault_score\": psi_val,\n",
    "#           \"mean_profile_cv_baseline\": mean_profile_cv_baseline\n",
    "#       }.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "480ce4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged metrics: {'drift_cohens_d': 0.3574004000931431, 'psi_fault_score': 0.14174696734949796, 'far_healthy': 0.006802721088435374, 'arl0_samples': 147.0, 'recent_pct_time_anomalous': 0.005059797608095676, 'mean_profile_cv_baseline': 1.74054039145304}\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6 (revised): Unlabeled evaluation ‚Äî Core-6 metrics only ---\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# 1) Composite fault score from profiles (z-normalized on baseline)\n",
    "profiles_df = pd.DataFrame(profile_series_map).sort_index()\n",
    "n = len(profiles_df)\n",
    "split_idx = int(n * BASELINE_FRACTION)\n",
    "\n",
    "baseline_df = profiles_df.iloc[:split_idx].copy()\n",
    "recent_df   = profiles_df.iloc[split_idx:].copy()\n",
    "\n",
    "baseline_means = baseline_df.mean(skipna=True)\n",
    "baseline_stds  = baseline_df.std(skipna=True).replace(0, np.nan)\n",
    "\n",
    "z_df     = (profiles_df - baseline_means) / baseline_stds\n",
    "z_pos_df = z_df.clip(lower=0)\n",
    "\n",
    "if AGGREGATION == \"mean_z\":\n",
    "    fault_score = z_pos_df.mean(axis=1, skipna=True)\n",
    "else:  # default \"max_z\"\n",
    "    fault_score = z_pos_df.max(axis=1, skipna=True)\n",
    "\n",
    "baseline_scores = fault_score.iloc[:split_idx].dropna()\n",
    "recent_scores   = fault_score.iloc[split_idx:].dropna()\n",
    "\n",
    "# 2) Threshold from baseline\n",
    "if THRESHOLD_METHOD == \"p99\":\n",
    "    thr = float(np.nanpercentile(baseline_scores, 99))\n",
    "else:  # \"mean+3sigma\"\n",
    "    thr = float(baseline_scores.mean() + 3 * baseline_scores.std())\n",
    "\n",
    "mlbase.log_params({\"eval_decision_threshold\": thr})\n",
    "\n",
    "# 3) Helper funcs\n",
    "def cohens_d(a, b):\n",
    "    a, b = np.asarray(a, float), np.asarray(b, float)\n",
    "    a, b = a[~np.isnan(a)], b[~np.isnan(b)]\n",
    "    if a.size < 2 or b.size < 2: return float(\"nan\")\n",
    "    m1, m2 = a.mean(), b.mean()\n",
    "    s = np.sqrt(((a.var(ddof=1) + b.var(ddof=1)) / 2.0))\n",
    "    return float((m2 - m1) / s) if s > 0 else float(\"nan\")\n",
    "\n",
    "def psi(a, b, bins=10):\n",
    "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
    "    a = a[~np.isnan(a)]; b = b[~np.isnan(b)]\n",
    "    if a.size < 10 or b.size < 10: return float(\"nan\")\n",
    "    qs = np.quantile(a, np.linspace(0,1,bins+1)); qs[0], qs[-1] = -np.inf, np.inf\n",
    "    val = 0.0\n",
    "    for i in range(bins):\n",
    "        p = ((a>=qs[i]) & (a<qs[i+1])).mean()\n",
    "        q = ((b>=qs[i]) & (b<qs[i+1])).mean()\n",
    "        p = max(p,1e-6); q = max(q,1e-6)\n",
    "        val += (q-p) * np.log(q/p)\n",
    "    return float(val)\n",
    "\n",
    "# 4) Core-6 metrics\n",
    "cohen_d = cohens_d(baseline_scores, recent_scores)          # separability\n",
    "psi_val = psi(baseline_scores, recent_scores, bins=10)      # distribution shift\n",
    "far_healthy = float(np.mean(baseline_scores >= thr))        # false alarms on baseline\n",
    "arl0 = float(1.0 / max(far_healthy, 1e-9))                  # expected samples between false alarms\n",
    "recent_flags = (recent_scores >= thr).astype(int)\n",
    "pct_time_anom_recent = float(recent_flags.mean())           # % anomalous in recent\n",
    "# mean CV across profiles on baseline\n",
    "cv_vals = []\n",
    "for p in baseline_df.columns:\n",
    "    mu = baseline_df[p].mean(skipna=True)\n",
    "    sd = baseline_df[p].std(skipna=True)\n",
    "    cv = (sd/mu) if (mu not in [0, np.nan] and not np.isnan(mu)) else np.nan\n",
    "    if not np.isnan(cv): cv_vals.append(cv)\n",
    "mean_profile_cv_baseline = float(np.mean(cv_vals)) if cv_vals else float(\"nan\")\n",
    "\n",
    "# Log ONLY the Core-6\n",
    "mlbase.log_metrics({\n",
    "    \"drift_cohens_d\": cohen_d,\n",
    "    \"psi_fault_score\": psi_val,\n",
    "    \"far_healthy\": far_healthy,\n",
    "    \"arl0_samples\": arl0,\n",
    "    \"recent_pct_time_anomalous\": pct_time_anom_recent,\n",
    "    \"mean_profile_cv_baseline\": mean_profile_cv_baseline\n",
    "})\n",
    "\n",
    "print(\"Logged metrics:\",\n",
    "      {k: v for k, v in {\n",
    "          \"drift_cohens_d\": cohen_d,\n",
    "          \"psi_fault_score\": psi_val,\n",
    "          \"far_healthy\": far_healthy,\n",
    "          \"arl0_samples\": arl0,\n",
    "          \"recent_pct_time_anomalous\": pct_time_anom_recent,\n",
    "          \"mean_profile_cv_baseline\": mean_profile_cv_baseline\n",
    "      }.items()})\n",
    "\n",
    "# Optional: A single quality index to sort runs (comment in if you want)\n",
    "# quality_index = (\n",
    "#     (cohen_d)\n",
    "#     - 5.0 * (far_healthy)\n",
    "#     + 0.5 * (psi_val if not np.isnan(psi_val) else 0.0)\n",
    "#     - 0.5 * (mean_profile_cv_baseline if not np.isnan(mean_profile_cv_baseline) else 0.0)\n",
    "# )\n",
    "# mlbase.log_metrics({\"quality_index\": float(quality_index)})\n",
    "# print(\"quality_index:\", quality_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e00c1d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality_index: -0.4760099174008047\n"
     ]
    }
   ],
   "source": [
    "# A single quality index to sort runs \n",
    "quality_index = (\n",
    "    (cohen_d)\n",
    "    - 5.0 * (far_healthy)\n",
    "    + 0.5 * (psi_val if not np.isnan(psi_val) else 0.0)\n",
    "    - 0.5 * (mean_profile_cv_baseline if not np.isnan(mean_profile_cv_baseline) else 0.0)\n",
    ")\n",
    "mlbase.log_metrics({\"quality_index\": float(quality_index)})\n",
    "print(\"quality_index:\", quality_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fd17080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell X: Attach rich run documentation (Notes) ---\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "def _fmt(x, nd=4):\n",
    "    if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):\n",
    "        return \"NaN\"\n",
    "    try:\n",
    "        return f\"{float(x):.{nd}f}\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "mlflow.set_tag(\n",
    "    \"mlflow.note.content\",\n",
    "f\"\"\"\n",
    "### Run Documentation\n",
    "\n",
    "This note summarizes **what we logged** in MLflow for quick comparison across runs and why each item matters.\n",
    "\n",
    "---\n",
    "\n",
    "#### üì¶ Artifact (the deliverable)\n",
    "- **Config YAML**: `{output_yaml}`    \n",
    "  This is the file used by the online realtime detector.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Parameters (setup & reproducibility)\n",
    "- **algorithm_name**: `{RUN_NAME}` ‚Äî approach used to generate the config.\n",
    "- **tenant_id / machine_id**: `{tenant_id}` / `{machine_id}` ‚Äî routing & traceability.\n",
    "- **dataset_path**: `{os.path.basename(dataset_path)}` ‚Äî source data identifier.\n",
    "- **stoppage_current_threshold**: `{stoppage_current_threshold}` ‚Äî filters non-operating periods.\n",
    "- **harmonic buckets**: low `{low_order_range}`, intermediate `{intermediate_order_range}`, high `{high_order_range}`.\n",
    "- **use_parity_profiles**: `{use_parity_profiles}` ‚Äî include odd/even profiles.\n",
    "- **eval_baseline_fraction**: `{BASELINE_FRACTION}` ‚Äî portion assumed healthy to derive baseline.\n",
    "- **eval_aggregation**: `{AGGREGATION}` ‚Äî how profiles combine into one fault score.\n",
    "- **eval_threshold_method**: `{THRESHOLD_METHOD}` ‚Äî rule for decision threshold on baseline.\n",
    "- **profiles_used**: `{profiles_used}` ‚Äî number of profiles with usable data.\n",
    "- **eval_decision_threshold**: `{_fmt(thr)}` ‚Äî derived operating threshold for the fault score.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä Core Metrics (for benchmarking without labels)\n",
    "- **missing_fraction_overall**: `{_fmt(missing_fraction_overall)}` ‚Äî overall data completeness.\n",
    "- **coverage_valid_row_fraction_min**: `{_fmt(coverage_min)}` ‚Äî worst coverage across profiles.\n",
    "\n",
    "**Separability & shift**\n",
    "- **drift_cohens_d**: `{_fmt(cohen_d)}` ‚Äî effect size between recent vs baseline fault scores.\n",
    "- **psi_fault_score**: `{_fmt(psi_val)}` ‚Äî population stability index for fault score.\n",
    "\n",
    "**False-alarm behavior**\n",
    "- **far_healthy**: `{_fmt(far_healthy)}` ‚Äî fraction of baseline flagged.\n",
    "- **arl0_samples**: `{_fmt(arl0)}` ‚Äî expected samples between false alarms.\n",
    "\n",
    "**Operational recent behavior**\n",
    "- **recent_pct_time_anomalous**: `{_fmt(pct_time_anom_recent)}` ‚Äî fraction of recent period flagged.\n",
    "\n",
    "**Baseline stability**\n",
    "- **mean_profile_cv_baseline**: `{_fmt(mean_profile_cv_baseline)}` ‚Äî average coefficient of variation across profiles in healthy window.\n",
    "\n",
    "**Single ranking score**\n",
    "- **quality_index**: `{_fmt(quality_index)}`  \n",
    "  Formula: `cohen_d - 5*far_healthy + 0.5*psi - 0.5*mean_profile_cv_baseline`.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Promotion guideline\n",
    "Promote when:\n",
    "- `far_healthy < 0.01` and `arl0_samples ‚â• 100`, **and**\n",
    "- `drift_cohens_d ‚â• 0.8`, **and**\n",
    "- `coverage_valid_row_fraction_min ‚â• 0.8`, **and**\n",
    "- `quality_index` ranks near the top among runs for this machine/config family.\n",
    "\n",
    "*Aggregation:* `{AGGREGATION}` ¬∑ *Thresholding:* `{THRESHOLD_METHOD}` ¬∑ *Baseline fraction:* `{BASELINE_FRACTION}`\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "264aea5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>low_order_current_harmonics</th>\n",
       "      <th>intermediate_order_current_harmonics</th>\n",
       "      <th>high_order_current_harmonics</th>\n",
       "      <th>odd_parity_current_harmonics</th>\n",
       "      <th>even_parity_current_harmonics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14491.000000</td>\n",
       "      <td>14491.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14491.000000</td>\n",
       "      <td>14491.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.521031</td>\n",
       "      <td>0.092680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027823</td>\n",
       "      <td>12.519126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.153551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043372</td>\n",
       "      <td>16.920934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.048286</td>\n",
       "      <td>0.158427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.061778</td>\n",
       "      <td>35.374847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.307929</td>\n",
       "      <td>1.712915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.876519</td>\n",
       "      <td>36.245903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       low_order_current_harmonics  intermediate_order_current_harmonics  \\\n",
       "count                 14491.000000                          14491.000000   \n",
       "mean                      0.521031                              0.092680   \n",
       "std                       0.783582                              0.153551   \n",
       "min                       0.000000                              0.000000   \n",
       "25%                       0.000000                              0.000000   \n",
       "50%                       0.000000                              0.000000   \n",
       "75%                       1.048286                              0.158427   \n",
       "max                       6.307929                              1.712915   \n",
       "\n",
       "       high_order_current_harmonics  odd_parity_current_harmonics  \\\n",
       "count                           0.0                  14491.000000   \n",
       "mean                            NaN                      0.027823   \n",
       "std                             NaN                      0.043372   \n",
       "min                             NaN                      0.000000   \n",
       "25%                             NaN                      0.000000   \n",
       "50%                             NaN                      0.000000   \n",
       "75%                             NaN                      0.061778   \n",
       "max                             NaN                      0.876519   \n",
       "\n",
       "       even_parity_current_harmonics  \n",
       "count                   14491.000000  \n",
       "mean                       12.519126  \n",
       "std                        16.920934  \n",
       "min                         0.000000  \n",
       "25%                         0.000000  \n",
       "50%                         0.000000  \n",
       "75%                        35.374847  \n",
       "max                        36.245903  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold (baseline-derived): 3.74718309784166\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 7: Quick sanity view (optional) ---\n",
    "check_df = pd.DataFrame(profile_series_map)\n",
    "display(check_df.describe())\n",
    "print(\"Threshold (baseline-derived):\", thr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a82dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run harmonic_profiling_kstest_v1 at: https://mlops.zolnoi.app/#/experiments/10/runs/5c73be7497194817bac1a56ba181dd52\n",
      "üß™ View experiment at: https://mlops.zolnoi.app/#/experiments/10\n",
      "MLflow run ended.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: End MLflow run ---\n",
    "mlflow.end_run()\n",
    "print(\"MLflow run ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceac3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
